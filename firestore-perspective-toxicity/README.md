# Analyze Toxicity with Perspective API

**Author**: Jigsaw's Perspective API (**[https://perspectiveapi.com](https://perspectiveapi.com/)**)

**Description**: We‚Äôve partnered with the Jigsaw team to build the Analyze Toxicity extension, which leverages machine learning to classify the level of toxicity, threat and profanity of your user comments. Perspective API (**[https://perspectiveapi.com](https://perspectiveapi.com/)**) uses the power of machine learning to help mitigate toxicity and ensure healthy dialogue online. It is trusted by platforms like the New York Times and Reddit. Please check it out [here](https://github.com/conversationai/firestore-perspective-toxicity).

---

## üß© Install this experimental extension

> ‚ö†Ô∏è **Experimental**: This extension is available for testing as an _experimental_ release. It has not been as thoroughly tested as the officially released extensions, and future updates might introduce breaking changes. If you use this extension, please [report bugs and make feature requests](https://github.com/conversationai/firestore-perspective-toxicity/issues/new/choose) in their GitHub repository.

**To install this extension visit the repository [conversationai/firestore-perspective-toxicity](https://github.com/conversationai/firestore-perspective-toxicity)**
